{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":620468,"sourceType":"datasetVersion","datasetId":304145},{"sourceId":8546728,"sourceType":"datasetVersion","datasetId":5106540}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-19T10:47:54.078163Z","iopub.execute_input":"2024-06-19T10:47:54.078525Z","iopub.status.idle":"2024-06-19T10:47:54.447152Z","shell.execute_reply.started":"2024-06-19T10:47:54.078497Z","shell.execute_reply":"2024-06-19T10:47:54.446183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download TorchVision repo to use some files from\n# references/detection\n!pip install pycocotools --quiet\n!git clone https://github.com/pytorch/vision.git\n!git checkout v0.3.0\n\n!cp vision/references/detection/utils.py ./\n!cp vision/references/detection/transforms.py ./\n!cp vision/references/detection/coco_eval.py ./\n!cp vision/references/detection/engine.py ./\n!cp vision/references/detection/coco_utils.py ./","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:47:54.479162Z","iopub.execute_input":"2024-06-19T10:47:54.479614Z","iopub.status.idle":"2024-06-19T10:49:23.196230Z","shell.execute_reply.started":"2024-06-19T10:47:54.479588Z","shell.execute_reply":"2024-06-19T10:49:23.194909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Basic python and ML Libraries\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\n# for ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# We will be reading images using OpenCV\nimport cv2\n\n# xml library for parsing xml files\nfrom xml.etree import ElementTree as et\n\n# matplotlib for visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# torchvision libraries\nimport torch\nfrom torch.utils.data import ConcatDataset\n\nfrom torch.utils.data import Subset\nimport torchvision\nfrom torchvision import transforms as torchtrans  \nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# these are the helper libraries imported.\nfrom engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n# for image augmentations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:53:35.675445Z","iopub.execute_input":"2024-06-19T10:53:35.676184Z","iopub.status.idle":"2024-06-19T10:53:35.683140Z","shell.execute_reply.started":"2024-06-19T10:53:35.676152Z","shell.execute_reply":"2024-06-19T10:53:35.682169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir -p /kaggle/working/train-inriaperson && cp -r /kaggle/input/inriaperson/Train/Annotations/* /kaggle/working/train-inriaperson","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:28.819017Z","iopub.execute_input":"2024-06-19T10:49:28.819983Z","iopub.status.idle":"2024-06-19T10:49:31.476792Z","shell.execute_reply.started":"2024-06-19T10:49:28.819945Z","shell.execute_reply":"2024-06-19T10:49:31.475574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cp -r /kaggle/input/inriaperson/Train/JPEGImages/* /kaggle/working/train-inriaperson","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:31.479387Z","iopub.execute_input":"2024-06-19T10:49:31.479698Z","iopub.status.idle":"2024-06-19T10:49:38.141838Z","shell.execute_reply.started":"2024-06-19T10:49:31.479673Z","shell.execute_reply":"2024-06-19T10:49:38.140651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir -p /kaggle/working/test-inriaperson && cp -r /kaggle/input/inriaperson/Test/Annotations/* /kaggle/working/test-inriaperson","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:38.143377Z","iopub.execute_input":"2024-06-19T10:49:38.143757Z","iopub.status.idle":"2024-06-19T10:49:39.904893Z","shell.execute_reply.started":"2024-06-19T10:49:38.143722Z","shell.execute_reply":"2024-06-19T10:49:39.903813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cp -r /kaggle/input/inriaperson/Test/JPEGImages/* /kaggle/working/test-inriaperson","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:39.906590Z","iopub.execute_input":"2024-06-19T10:49:39.907495Z","iopub.status.idle":"2024-06-19T10:49:43.513301Z","shell.execute_reply.started":"2024-06-19T10:49:39.907450Z","shell.execute_reply":"2024-06-19T10:49:43.512058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining the files directory and testing directory\ntrain_dir = '/kaggle/working/train-inriaperson'\ntest_dir  = '/kaggle/working/test-inriaperson'","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:43.514759Z","iopub.execute_input":"2024-06-19T10:49:43.515071Z","iopub.status.idle":"2024-06-19T10:49:43.519681Z","shell.execute_reply.started":"2024-06-19T10:49:43.515045Z","shell.execute_reply":"2024-06-19T10:49:43.518817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"add_train_dir = '/kaggle/input/pedestriansyntheticdataset/train/train'\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:43.520831Z","iopub.execute_input":"2024-06-19T10:49:43.521098Z","iopub.status.idle":"2024-06-19T10:49:43.531245Z","shell.execute_reply.started":"2024-06-19T10:49:43.521075Z","shell.execute_reply":"2024-06-19T10:49:43.530253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImagesDataset(torch.utils.data.Dataset):\n\n    def __init__(self, files_dir, width, height, transforms=None):\n        self.transforms = transforms\n        self.files_dir = files_dir\n        self.height = height\n        self.width = width\n            \n        self.imgs = [image for image in sorted(os.listdir(files_dir)) if image.endswith(('.png', '.jpg'))]\n        \n        \n        # classes: 0 index is reserved for background\n        self.classes = ['background','pedestrian']\n\n    def __getitem__(self, idx):\n\n        img_name = self.imgs[idx]\n        image_path = os.path.join(self.files_dir, img_name)\n\n        # reading the images and converting them to correct size and color    \n        img = cv2.imread(image_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n        # diving by 255\n        img_res /= 255.0\n        \n        # annotation file\n        annot_filename = img_name[:-4] + '.xml'\n        annot_file_path = os.path.join(self.files_dir, annot_filename)\n        \n        boxes = []\n        labels = []\n        tree = et.parse(annot_file_path)\n        root = tree.getroot()\n        \n        # cv2 image gives size as height x width\n        wt = img.shape[1]\n        ht = img.shape[0]\n        \n        # box coordinates for xml files are extracted and corrected for image size given\n        for member in root.findall('object'):\n            label_text = member.find('name').text.lower()\n            if label_text in ['ped', 'pedestrian','person']:\n                labels.append(self.classes.index('pedestrian')) \n            \n            # bounding box\n            xmin = int(member.find('bndbox').find('xmin').text)\n            xmax = int(member.find('bndbox').find('xmax').text)\n            \n            ymin = int(member.find('bndbox').find('ymin').text)\n            ymax = int(member.find('bndbox').find('ymax').text)\n            \n            \n            xmin_corr = (xmin/wt)*self.width\n            xmax_corr = (xmax/wt)*self.width\n            ymin_corr = (ymin/ht)*self.height\n            ymax_corr = (ymax/ht)*self.height\n            \n            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n        \n        # convert boxes into a torch.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        # getting the areas of the boxes\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        \n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        # image_id\n        image_id = idx\n        target[\"image_id\"] = image_id\n        \n  \n\n\n        if self.transforms:\n            \n            sample = self.transforms(image = img_res,\n                                     bboxes = target['boxes'],\n                                     labels = labels)\n            \n            img_res = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n            \n            \n        return img_res, target\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:43.532570Z","iopub.execute_input":"2024-06-19T10:49:43.532867Z","iopub.status.idle":"2024-06-19T10:49:43.553244Z","shell.execute_reply.started":"2024-06-19T10:49:43.532845Z","shell.execute_reply":"2024-06-19T10:49:43.552361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_object_detection_model(num_classes):\n\n    # load a model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    \n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:43.557141Z","iopub.execute_input":"2024-06-19T10:49:43.557427Z","iopub.status.idle":"2024-06-19T10:49:43.566952Z","shell.execute_reply.started":"2024-06-19T10:49:43.557400Z","shell.execute_reply":"2024-06-19T10:49:43.566068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Send train=True fro training transforms and False for val/test transforms\ndef get_transform(train):\n    \n    if train:\n        return A.Compose([\n            A.HorizontalFlip(0.5),\n            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n            A.Rotate(limit=15, p=0.5),\n            ToTensorV2(p=1.0)\n        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n    else:\n        return A.Compose([\n                            ToTensorV2(p=1.0)\n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:43.567957Z","iopub.execute_input":"2024-06-19T10:49:43.568235Z","iopub.status.idle":"2024-06-19T10:49:43.577277Z","shell.execute_reply.started":"2024-06-19T10:49:43.568203Z","shell.execute_reply":"2024-06-19T10:49:43.576403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use our dataset and defined transformations\ntrain_dataset = ImagesDataset(train_dir, 1920, 1080, transforms= get_transform(train=True))\ntest_dataset = ImagesDataset(test_dir, 1920, 1080, transforms= get_transform(train=False))","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:43.578421Z","iopub.execute_input":"2024-06-19T10:49:43.578726Z","iopub.status.idle":"2024-06-19T10:49:43.593964Z","shell.execute_reply.started":"2024-06-19T10:49:43.578703Z","shell.execute_reply":"2024-06-19T10:49:43.593309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adjust the length of test dataset to align with other train datasets\n# Define the total number of samples in your dataset\ntrain_total_samples = len(train_dataset)\n\n# Define the percentage of the dataset you want to load\ntrain_portion_to_load = 0.5  # Example: load 50% of the dataset\n\n# Calculate the number of samples to load\ntrain_num_samples_to_load = int(train_total_samples * train_portion_to_load)\n\n# Generate random indices to select a random subset of the dataset\n\ntrain_indices = random.sample(range(train_total_samples), train_num_samples_to_load)\n\n# Create a Subset dataset containing only the selected indices\ntrain_subset = Subset(train_dataset, train_indices)\nlen(train_subset)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:43.595014Z","iopub.execute_input":"2024-06-19T10:49:43.595700Z","iopub.status.idle":"2024-06-19T10:49:43.672761Z","shell.execute_reply.started":"2024-06-19T10:49:43.595675Z","shell.execute_reply":"2024-06-19T10:49:43.671931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adjust the length of test dataset to align with other test datasets\n# Define the total number of samples in your dataset\ntest_total_samples = len(test_dataset)\n\n# Define the percentage of the dataset you want to load\ntest_portion_to_load = 0.3  # Example: load 50% of the dataset\n\n# Calculate the number of samples to load\ntest_num_samples_to_load = int(test_total_samples * test_portion_to_load)\n\n# Generate random indices to select a random subset of the dataset\n\n\ntest_indices = random.sample(range(test_total_samples), test_num_samples_to_load)\n\n# Create a Subset dataset containing only the selected indices\ntest_subset = Subset(test_dataset, test_indices)\nlen(test_subset)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:49:43.673871Z","iopub.execute_input":"2024-06-19T10:49:43.674158Z","iopub.status.idle":"2024-06-19T10:49:43.754644Z","shell.execute_reply.started":"2024-06-19T10:49:43.674111Z","shell.execute_reply":"2024-06-19T10:49:43.753663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_subset \nadd_train_dataset = ImagesDataset(add_train_dir, 1920, 1080, transforms= get_transform(train=True))\ncombined_dataset = ConcatDataset([train_dataset, add_train_dataset])\ntest_dataset = test_subset\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:53:40.930774Z","iopub.execute_input":"2024-06-19T10:53:40.931474Z","iopub.status.idle":"2024-06-19T10:53:40.937797Z","shell.execute_reply.started":"2024-06-19T10:53:40.931442Z","shell.execute_reply":"2024-06-19T10:53:40.936944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(combined_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:53:53.335063Z","iopub.execute_input":"2024-06-19T10:53:53.335828Z","iopub.status.idle":"2024-06-19T10:53:53.341287Z","shell.execute_reply.started":"2024-06-19T10:53:53.335797Z","shell.execute_reply":"2024-06-19T10:53:53.340439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# define training and validation data loaders\n# data_loader_train = torch.utils.data.DataLoader(\n#     train_dataset, batch_size=4, shuffle=True, num_workers=4,\n#     collate_fn=utils.collate_fn)\n\ncombined_data_loader_train = torch.utils.data.DataLoader(\n    combined_dataset, batch_size=4, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\n\ndata_loader_test = torch.utils.data.DataLoader(\n    test_dataset, batch_size=4, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:54:27.951238Z","iopub.execute_input":"2024-06-19T10:54:27.951616Z","iopub.status.idle":"2024-06-19T10:54:27.957757Z","shell.execute_reply.started":"2024-06-19T10:54:27.951588Z","shell.execute_reply":"2024-06-19T10:54:27.956686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to train on gpu if selected.\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_object_detection_model(num_classes)\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:54:34.777921Z","iopub.execute_input":"2024-06-19T10:54:34.778309Z","iopub.status.idle":"2024-06-19T10:54:36.998599Z","shell.execute_reply.started":"2024-06-19T10:54:34.778278Z","shell.execute_reply":"2024-06-19T10:54:36.997770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # training for one epoch\n    train_one_epoch(model, optimizer, combined_data_loader_train, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:54:52.810017Z","iopub.execute_input":"2024-06-19T10:54:52.810705Z","iopub.status.idle":"2024-06-19T11:14:03.545511Z","shell.execute_reply.started":"2024-06-19T10:54:52.810675Z","shell.execute_reply":"2024-06-19T11:14:03.544247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}